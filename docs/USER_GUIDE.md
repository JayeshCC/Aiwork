# üìò AIWork User Guide

Welcome to **AIWork** - the lightweight, production-ready framework for building intelligent agentic AI applications. This comprehensive guide will take you from beginner to expert, teaching you how to build sophisticated AI workflows that run efficiently on Intel¬Æ hardware.

---

## Table of Contents

1. [Introduction](#1-introduction)
2. [Core Concepts](#2-core-concepts)
3. [Getting Started](#3-getting-started)
4. [Core Features](#4-core-features)
5. [Advanced Topics](#5-advanced-topics)
6. [Best Practices](#6-best-practices)
7. [Troubleshooting](#7-troubleshooting)
8. [FAQ](#8-faq)

---

## 1. üåü Introduction

### What is AIWork?

AIWork is a Python framework that lets you build intelligent systems using the **agent paradigm**. Instead of writing monolithic scripts, you define autonomous "agents" with specific roles, give them tools, and orchestrate their work through task flows.

Think of it as building your own **LangChain** or **CrewAI** from scratch, but lighter, faster, and optimized for Intel hardware.

### Why Use AIWork?

**For Developers:**
- ‚úÖ **No Vendor Lock-in**: You own the code, no proprietary frameworks
- ‚úÖ **Lightweight**: Minimal dependencies, easy to understand
- ‚úÖ **Production Ready**: Built-in retry logic, error handling, state management
- ‚úÖ **Fast**: Intel OpenVINO integration for ML acceleration

**For Teams:**
- ‚úÖ **Clear Architecture**: Agents, Tasks, Flows - easy to reason about
- ‚úÖ **Scalable**: Start simple, add complexity as needed
- ‚úÖ **Testable**: Each component can be tested independently
- ‚úÖ **Maintainable**: Clean separation of concerns

**For Intel Users:**
- ‚úÖ **Hardware Optimized**: Built for Intel CPUs and GPUs
- ‚úÖ **OpenVINO Ready**: Seamless integration for ML workloads
- ‚úÖ **DevCloud Compatible**: Tested on Intel DevCloud infrastructure

### When to Use AIWork?

**Perfect For:**
- Document processing pipelines (OCR, extraction, analysis)
- Customer support automation (triage, routing, response)
- Data analysis workflows (extract, transform, analyze)
- Research automation (search, summarize, report)
- Business process automation (multi-step workflows)

**Not Ideal For:**
- Simple scripts (use plain Python)
- Real-time gaming (use game engines)
- Ultra-low latency requirements (<1ms)
- Systems where you need CrewAI's specific features

---

## 2. üß† Core Concepts

AIWork is built on four foundational concepts: **Agents**, **Tasks**, **Flows**, and the **Orchestrator**.

### 2.1 Agents ü§ñ

An **Agent** is an autonomous AI worker with a personality and capabilities.

**Anatomy of an Agent:**

```python
from aiwork.core.agent import Agent

analyst = Agent(
    role="Financial Analyst",              # Job title
    goal="Detect fraud in transactions",   # Primary objective
    backstory="20 years at Big Bank",      # Context for behavior
    tools=[calculator, database],          # Available capabilities
    memory=vector_memory,                  # Optional context storage
    verbose=True                           # Show thinking process
)
```

**Key Properties:**
- **Role**: Defines the agent's identity (e.g., "Data Scientist", "Customer Service Rep")
- **Goal**: What the agent is trying to achieve
- **Backstory**: Additional context that influences decision-making
- **Tools**: List of functions/objects the agent can use
- **Memory**: Optional memory system for storing and recalling context

**How Agents Work:**

When you assign a task to an agent:
1. Agent reviews its **goal** and **backstory**
2. Checks its **memory** for relevant context
3. Selects appropriate **tools** to complete the task
4. Executes the task with its personality/approach
5. Returns the result

**Example Use Cases:**

```python
# Document Processing Agent
ocr_agent = Agent(
    role="Document Processor",
    goal="Extract accurate text from images",
    backstory="Specialist in OCR with 95% accuracy",
    tools=[openvino_ocr]
)

# Customer Support Agent
support_agent = Agent(
    role="Support Specialist",
    goal="Resolve customer issues quickly",
    backstory="Friendly helper with product expertise",
    tools=[knowledge_base, ticket_system]
)

# Data Analyst Agent
analyst_agent = Agent(
    role="Data Analyst",
    goal="Find insights in data",
    backstory="Expert at pattern recognition",
    tools=[pandas_tool, visualization_tool]
)
```

### 2.2 Tasks üìã

A **Task** is an atomic unit of work. It's what you ask an agent to do.

**Creating Tasks:**

```python
from aiwork.core.task import Task

# Simple function-based task
def extract_text(ctx):
    document = ctx["document"]
    return {"text": perform_ocr(document)}

task1 = Task(
    name="extract",
    description="Extract text from invoice",
    handler=extract_text,
    retries=3,
    guardrails=[]
)

# Agent-based task
task2 = Task(
    name="analyze",
    description="Analyze extracted text for fraud",
    agent=analyst_agent,
    handler=analyze_handler,
    retries=2
)
```

**Task Execution Flow:**

1. **Setup**: Task receives context from orchestrator
2. **Execution**: Handler function runs (or agent executes)
3. **Validation**: Guardrails check the output
4. **Retry**: If failed, retry up to configured limit
5. **Completion**: Result stored in context

**Task Context:**

Every task receives a `context` dictionary containing:

```python
{
    "outputs": {
        "previous_task": {"result": "data"},
        "another_task": {"value": 123}
    },
    "inputs": {
        # Initial inputs to the flow
    }
}
```

**Task Dependencies:**

```python
flow.add_task(task_a)                      # No dependencies
flow.add_task(task_b, depends_on=["task_a"])  # Runs after task_a
flow.add_task(task_c, depends_on=["task_a", "task_b"])  # Runs after both
```

### 2.3 Flows üîÄ

A **Flow** is a workflow - a directed acyclic graph (DAG) of tasks.

**Creating Flows:**

```python
from aiwork.core.flow import Flow

flow = Flow("invoice_processing")

# Add tasks in any order
flow.add_task(extract_task)
flow.add_task(validate_task, depends_on=["extract"])
flow.add_task(analyze_task, depends_on=["extract"])
flow.add_task(report_task, depends_on=["validate", "analyze"])
```

**Visualization:**

```
extract_task
     ‚Üì
  ‚îå‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îê
  ‚Üì     ‚Üì
validate  analyze
  ‚Üì     ‚Üì
  ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò
     ‚Üì
  report_task
```

**Flow Properties:**

- **DAG Structure**: No cycles allowed (prevents infinite loops)
- **Dependency Resolution**: Automatically determines execution order
- **Context Passing**: Outputs flow from task to task
- **Error Handling**: Failed tasks can stop the flow or be retried

**Static vs Dynamic Flows:**

**Static Flow** (predefined):
```python
flow = Flow("static_pipeline")
flow.add_task(task_a)
flow.add_task(task_b, depends_on=["task_a"])
flow.add_task(task_c, depends_on=["task_b"])
# Execution order is fixed: A ‚Üí B ‚Üí C
```

**Dynamic Flow** (agent-modified):
```python
def smart_task_handler(ctx):
    result = process_data(ctx)
    
    # Agent decides to inject new task
    if result["requires_approval"]:
        approval_task = Task("approval", approval_handler)
        result["next_tasks"] = [approval_task]  # Injected dynamically!
    
    return result
```

### 2.4 Orchestrator üéº

The **Orchestrator** is the execution engine that runs your flows.

**Basic Usage:**

```python
from aiwork.orchestrator import Orchestrator

orchestrator = Orchestrator()

# Execute a flow
result = orchestrator.execute(
    flow=my_flow,
    initial_context={"user_id": "12345", "document": doc}
)

# Access outputs
print(result["outputs"])
```

**What the Orchestrator Does:**

1. **Topological Sort**: Determines correct execution order from DAG
2. **Sequential Execution**: Runs tasks one at a time (in correct order)
3. **Context Management**: Passes data between tasks
4. **Dynamic Injection**: Handles tasks added at runtime
5. **Error Recovery**: Manages task failures and retries

**Execution Flow:**

```
1. Orchestrator receives Flow + initial context
2. Resolves DAG into execution order
3. For each task in order:
   a. Check dependencies completed
   b. Execute task with current context
   c. Store output in context
   d. Check for dynamic tasks to inject
4. Return final context with all outputs
```

---

## 3. üöÄ Getting Started

### 3.1 Installation

**Prerequisites:**
- Python 3.8 or higher
- pip (Python package manager)
- Virtual environment (recommended)

**Step 1: Clone the Repository**

```bash
git clone https://github.com/JayeshCC/Aiwork.git
cd Aiwork
```

**Step 2: Create Virtual Environment**

```bash
# Create environment
python3 -m venv .venv

# Activate (Linux/Mac)
source .venv/bin/activate

# Activate (Windows)
.venv\Scripts\activate
```

**Step 3: Install Dependencies**

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

**Step 4: Verify Installation**

```bash
python examples/quickstart.py
```

You should see output showing a document processing pipeline executing successfully.

### 3.2 Your First Agent

Let's build a simple research agent that searches for information and summarizes it.

**Step 1: Create the Script**

```python
# my_first_agent.py
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from aiwork.core.agent import Agent
from aiwork.core.task import Task
from aiwork.core.flow import Flow
from aiwork.orchestrator import Orchestrator

# Define the research agent
researcher = Agent(
    role="Research Analyst",
    goal="Find and summarize relevant information",
    backstory="Expert researcher with 10 years experience"
)

# Define task handlers
def search_web(ctx):
    query = ctx.get("query", "AI trends")
    print(f"Searching for: {query}")
    # In real implementation, this would call a search API
    return {
        "results": [
            "AI adoption is growing 40% year-over-year",
            "OpenVINO accelerates AI inference on Intel hardware",
            "Agentic frameworks are the future of AI development"
        ]
    }

def summarize_results(ctx):
    results = ctx["outputs"]["search"]["results"]
    print(f"Summarizing {len(results)} results...")
    summary = f"Found {len(results)} key insights about the topic."
    return {"summary": summary, "details": results}

# Build the flow
flow = Flow("research_pipeline")

search_task = Task(
    name="search",
    description="Search for information",
    agent=researcher,
    handler=search_web
)

summary_task = Task(
    name="summarize",
    description="Summarize findings",
    agent=researcher,
    handler=summarize_results
)

flow.add_task(search_task)
flow.add_task(summary_task, depends_on=["search"])

# Execute
orchestrator = Orchestrator()
result = orchestrator.execute(flow, {"query": "Intel AI trends"})

print("\n=== Final Results ===")
print(f"Summary: {result['outputs']['summarize']['summary']}")
print(f"Details: {result['outputs']['summarize']['details']}")
```

**Step 2: Run It**

```bash
python my_first_agent.py
```

**Expected Output:**

```
Starting Flow: research_pipeline
  Executing Task: search...
    > Assigned to Agent: Research Analyst
ü§ñ [Agent: Research Analyst]
   Goal: Find and summarize relevant information
   Working on: Search for information
Searching for: Intel AI trends
  Task search Completed.
  Executing Task: summarize...
    > Assigned to Agent: Research Analyst
ü§ñ [Agent: Research Analyst]
   Goal: Find and summarize relevant information
   Working on: Summarize findings
Summarizing 3 results...
  Task summarize Completed.
Flow research_pipeline Finished.

=== Final Results ===
Summary: Found 3 key insights about the topic.
Details: ['AI adoption is growing 40% year-over-year', ...]
```

### 3.3 Basic Workflow

The typical AIWork development workflow:

1. **Define Agents** - Create your AI workers
2. **Create Tasks** - Define what needs to be done
3. **Build Flow** - Connect tasks with dependencies
4. **Execute** - Run through orchestrator
5. **Review Results** - Check outputs and refine

---

## 4. üîß Core Features

### 4.1 Task System

The task system is the heart of AIWork's execution model.

**Basic Task Creation:**

```python
def my_handler(ctx):
    # Access previous outputs
    data = ctx["outputs"].get("previous_task", {})
    
    # Do work
    result = process(data)
    
    # Return result
    return {"processed_data": result}

task = Task(
    name="my_task",
    description="Process data",
    handler=my_handler,
    retries=3
)
```

**Task with Retry Logic:**

```python
def unreliable_api_call(ctx):
    import random
    if random.random() < 0.3:  # 30% failure rate
        raise Exception("API timeout")
    return {"data": "success"}

task = Task(
    name="api_call",
    handler=unreliable_api_call,
    retries=5  # Will retry up to 5 times
)
```

**Task Lifecycle:**

```
PENDING ‚Üí RUNNING ‚Üí COMPLETED
                 ‚Üò FAILED (after all retries)
```

**Accessing Task Outputs:**

```python
# In a task handler
def dependent_task(ctx):
    # Access outputs from previous tasks
    extracted_text = ctx["outputs"]["extract_task"]["text"]
    validated_data = ctx["outputs"]["validate_task"]["data"]
    
    # Use them
    result = analyze(extracted_text, validated_data)
    return {"analysis": result}
```

### 4.2 DAG Workflows

Direct Acyclic Graphs (DAGs) define task execution order.

**Linear Flow:**

```python
flow = Flow("linear")
flow.add_task(Task("step1", handler1))
flow.add_task(Task("step2", handler2), depends_on=["step1"])
flow.add_task(Task("step3", handler3), depends_on=["step2"])

# Execution: step1 ‚Üí step2 ‚Üí step3
```

**Parallel Branches:**

```python
flow = Flow("parallel")
flow.add_task(Task("start", start_handler))

# Two parallel branches after start
flow.add_task(Task("branch_a", handler_a), depends_on=["start"])
flow.add_task(Task("branch_b", handler_b), depends_on=["start"])

# Join branches
flow.add_task(Task("end", end_handler), depends_on=["branch_a", "branch_b"])

# Execution:
#        start
#       ‚Üô    ‚Üò
#  branch_a  branch_b
#       ‚Üò    ‚Üô
#         end
```

**Diamond Pattern:**

```python
flow = Flow("diamond")
flow.add_task(Task("extract", extract_handler))
flow.add_task(Task("validate", validate_handler), depends_on=["extract"])
flow.add_task(Task("analyze", analyze_handler), depends_on=["extract"])
flow.add_task(Task("report", report_handler), depends_on=["validate", "analyze"])

#     extract
#      ‚Üô    ‚Üò
# validate  analyze
#      ‚Üò    ‚Üô
#      report
```

**Complex DAG Example:**

```python
# Real-world document processing pipeline
flow = Flow("document_pipeline")

# Stage 1: Ingestion
flow.add_task(Task("upload", upload_handler))

# Stage 2: Preprocessing (parallel)
flow.add_task(Task("ocr", ocr_handler), depends_on=["upload"])
flow.add_task(Task("metadata", metadata_handler), depends_on=["upload"])

# Stage 3: Analysis (parallel, depends on OCR)
flow.add_task(Task("sentiment", sentiment_handler), depends_on=["ocr"])
flow.add_task(Task("entities", entity_handler), depends_on=["ocr"])
flow.add_task(Task("topics", topic_handler), depends_on=["ocr"])

# Stage 4: Validation
flow.add_task(Task("validate", validate_handler), 
             depends_on=["sentiment", "entities", "topics", "metadata"])

# Stage 5: Storage
flow.add_task(Task("store", store_handler), depends_on=["validate"])
```

### 4.3 Hybrid Orchestration

**Static + Dynamic** task injection - the killer feature!

**Problem:** Traditional workflows are rigid. What if you need to add steps based on data?

**Solution:** Agents can inject new tasks at runtime.

**Example: Invoice Processing with Dynamic Compliance**

```python
def analyze_invoice(ctx):
    invoice_data = ctx["outputs"]["ocr"]["data"]
    amount = invoice_data["total"]
    vendor = invoice_data["vendor"]
    
    result = {
        "amount": amount,
        "vendor": vendor,
        "risk_score": calculate_risk(amount, vendor)
    }
    
    # Dynamic decision: high-value invoices need audit
    if amount > 10000:
        print("High-value invoice detected! Injecting audit task...")
        
        audit_task = Task(
            name="compliance_audit",
            description=f"Audit ${amount} payment to {vendor}",
            agent=compliance_agent,
            handler=audit_handler
        )
        
        # This task will be executed next!
        result["next_tasks"] = [audit_task]
    
    return result

# Original flow only has: OCR ‚Üí Analyze ‚Üí Store
flow = Flow("invoice_processing")
flow.add_task(Task("ocr", ocr_handler))
flow.add_task(Task("analyze", analyze_invoice), depends_on=["ocr"])
flow.add_task(Task("store", store_handler), depends_on=["analyze"])

# But if amount > 10000, the flow becomes:
# OCR ‚Üí Analyze ‚Üí Compliance_Audit ‚Üí Store (dynamically!)
```

**Example: Error Recovery**

```python
def risky_operation(ctx):
    try:
        result = attempt_operation()
        return {"status": "success", "data": result}
    except Exception as e:
        print("Operation failed, injecting retry with different strategy...")
        
        retry_task = Task(
            name="retry_with_fallback",
            handler=fallback_handler
        )
        
        return {
            "status": "failed",
            "error": str(e),
            "next_tasks": [retry_task]
        }
```

**Benefits:**
- ‚úÖ Adapts to data at runtime
- ‚úÖ No complex branching logic
- ‚úÖ Easier to understand and maintain
- ‚úÖ Agent-driven decision making

### 4.4 Memory & Context

Agents can remember past interactions using **VectorMemory**.

**Creating Memory:**

```python
from aiwork.core.memory import VectorMemory

memory = VectorMemory()

# Store information
memory.add("Customer prefers email communication")
memory.add("Customer is in premium tier")
memory.add("Previous issue was resolved on 2024-01-15")

# Create agent with memory
support_agent = Agent(
    role="Support Agent",
    goal="Provide personalized support",
    memory=memory
)
```

**How Memory Works:**

When an agent executes a task:
1. Agent receives task description
2. Searches memory for relevant context using similarity
3. Incorporates remembered information into decision
4. Executes task with full context

**Example with Memory:**

```python
from aiwork.core.memory import VectorMemory

# Create memory and add context
memory = VectorMemory()
memory.add("User is a Python developer")
memory.add("User prefers detailed technical explanations")
memory.add("User has experience with FastAPI")

# Create agent with memory
assistant = Agent(
    role="Technical Assistant",
    goal="Help users with coding questions",
    backstory="Expert software engineer",
    memory=memory
)

# When agent executes, it recalls relevant memories
def help_handler(ctx):
    question = ctx.get("question")
    # Agent automatically searches memory for context
    # and provides personalized response
    return {"answer": f"Technical answer for: {question}"}

task = Task(
    name="help",
    description="Answer: How do I optimize my API?",
    agent=assistant,
    handler=help_handler
)
```

**Memory Search Results:**

```python
# Searching memory
results = memory.search("API performance", top_k=3)
# Returns most relevant memories:
# 1. "User has experience with FastAPI"
# 2. "User prefers detailed technical explanations"
# 3. "User is a Python developer"
```

### 4.5 Guardrails

**Guardrails** validate task outputs to ensure compliance and quality.

**Creating Guardrails:**

```python
from aiwork.core.guardrail import Guardrail

# Simple validation
def check_positive(data):
    return data.get("amount", 0) > 0

positive_guardrail = Guardrail(
    name="positive_amount",
    validator=check_positive,
    description="Ensures amount is positive"
)

# Complex validation
def check_data_quality(data):
    required_fields = ["name", "email", "amount"]
    has_all_fields = all(field in data for field in required_fields)
    valid_email = "@" in data.get("email", "")
    return has_all_fields and valid_email

quality_guardrail = Guardrail(
    name="data_quality",
    validator=check_data_quality,
    description="Validates data completeness and format"
)
```

**Using Guardrails:**

```python
task = Task(
    name="process_payment",
    handler=payment_handler,
    guardrails=[positive_guardrail, quality_guardrail]
)

# If validation fails, task raises exception
# and can be retried (if retries > 0)
```

**Guardrail Examples:**

```python
# Security: Check for sensitive data
def no_sensitive_data(output):
    text = str(output).lower()
    forbidden = ["password", "ssn", "credit_card"]
    return not any(word in text for word in forbidden)

security_guard = Guardrail(
    name="security_check",
    validator=no_sensitive_data,
    description="Prevents leaking sensitive information"
)

# Business rule: Invoice amounts
def valid_invoice_amount(output):
    amount = output.get("amount", 0)
    return 0 < amount < 100000  # Between $0 and $100k

amount_guard = Guardrail(
    name="invoice_limit",
    validator=valid_invoice_amount,
    description="Ensures invoice is within limits"
)

# Format validation: Email
def valid_email_format(output):
    email = output.get("email", "")
    return "@" in email and "." in email.split("@")[1]

email_guard = Guardrail(
    name="email_format",
    validator=valid_email_format,
    description="Validates email format"
)
```

**Guardrail Execution Flow:**

```
Task executes ‚Üí Produces output
    ‚Üì
Guardrail 1: validate(output) ‚Üí Pass
    ‚Üì
Guardrail 2: validate(output) ‚Üí Pass
    ‚Üì
Guardrail 3: validate(output) ‚Üí FAIL!
    ‚Üì
Exception raised ‚Üí Task retries (if retries > 0)
```

---

## 5. üéì Advanced Topics

### 5.1 REST API Usage

Deploy your agents as a web service.

**Starting the Server:**

```bash
python -m aiwork.api.server
```

Server starts on `http://localhost:8000`

**API Endpoints:**

**1. Health Check**

```bash
curl http://localhost:8000/
```

Response:
```json
{
  "status": "healthy",
  "framework": "AIWork"
}
```

**2. Execute Flow**

```bash
curl -X POST http://localhost:8000/execute \
  -H "Content-Type: application/json" \
  -d '{
    "flow_name": "document_pipeline",
    "tasks": [
      {"name": "extract"},
      {"name": "analyze", "depends_on": ["extract"]},
      {"name": "store", "depends_on": ["analyze"]}
    ],
    "input_context": {
      "document_id": "DOC-12345",
      "user_id": "user@example.com"
    }
  }'
```

Response:
```json
{
  "status": "success",
  "outputs": {
    "extract": {"text": "..."},
    "analyze": {"insights": "..."},
    "store": {"stored": true}
  }
}
```

**Custom Server:**

```python
from fastapi import FastAPI
from aiwork.orchestrator import Orchestrator
from aiwork.core.flow import Flow

app = FastAPI()

@app.post("/run-analysis")
def run_analysis(document_url: str):
    # Build custom flow
    flow = Flow("custom_analysis")
    # ... add tasks ...
    
    # Execute
    orch = Orchestrator()
    result = orch.execute(flow, {"url": document_url})
    
    return {"result": result["outputs"]}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 5.2 Kafka Integration

Distributed task processing with Apache Kafka.

**Setup:**

```python
from aiwork.integrations.kafka_adapter import KafkaAdapter

# Connect to Kafka
kafka = KafkaAdapter(bootstrap_servers="localhost:9092")
```

**Producer (Send Tasks):**

```python
# Produce task to queue
task_payload = {
    "task_id": "task-001",
    "flow_name": "process_document",
    "inputs": {"document_id": "DOC-123"}
}

kafka.produce_task("aiwork.tasks", task_payload)
print("Task sent to Kafka!")
```

**Consumer (Process Tasks):**

```python
# Consume and process tasks
for task in kafka.consume_tasks("aiwork.tasks"):
    print(f"Received task: {task['task_id']}")
    
    # Build and execute flow
    flow = build_flow_from_task(task)
    orch = Orchestrator()
    result = orch.execute(flow, task["inputs"])
    
    # Send result back
    kafka.produce_task("aiwork.results", {
        "task_id": task["task_id"],
        "result": result["outputs"]
    })
```

**Distributed Architecture:**

```
Producer Service         Kafka Cluster         Worker Services
     ‚Üì                        ‚Üì                       ‚Üì
[Submit Tasks] ‚Üí [aiwork.tasks topic] ‚Üí [Worker 1: Process]
                                      ‚Üí [Worker 2: Process]
                                      ‚Üí [Worker 3: Process]
                 ‚Üì
           [aiwork.results topic]
                 ‚Üì
[Results Consumer Service]
```

**Note:** Current implementation is a stub. Full Kafka integration requires:
```bash
pip install confluent-kafka
```

### 5.3 OpenVINO Optimization

Accelerate ML models with Intel¬Æ OpenVINO‚Ñ¢.

**Basic Usage:**

```python
from aiwork.integrations.openvino_adapter import OpenVINOAdapter

# Load model
ov = OpenVINOAdapter(model_path="models/distilbert.xml")

# Run inference
result = ov.infer({
    "input_ids": token_ids,
    "attention_mask": attention_mask
})

print(f"Inference result: {result}")
print(f"Speedup: {result['speedup']}")  # ~3.7x on Intel CPUs
```

**With Agents:**

```python
# Create OpenVINO-powered OCR agent
ov_ocr = OpenVINOAdapter(model_path="models/ocr_model.xml")

ocr_agent = Agent(
    role="OCR Specialist",
    goal="Extract text from images",
    backstory="Uses Intel OpenVINO for fast inference",
    tools=[ov_ocr]
)

def ocr_handler(ctx):
    image = ctx["image"]
    result = ov_ocr.infer({"image": image})
    return {"text": result["text"]}

task = Task(
    name="ocr",
    description="Extract text from invoice image",
    agent=ocr_agent,
    handler=ocr_handler
)
```

**Performance Comparison:**

```python
import time

# Baseline (PyTorch)
start = time.time()
for _ in range(100):
    pytorch_model.predict(data)
pytorch_time = time.time() - start

# OpenVINO
start = time.time()
for _ in range(100):
    ov_adapter.infer(data)
openvino_time = time.time() - start

speedup = pytorch_time / openvino_time
print(f"Speedup: {speedup:.2f}x")  # Typically 3-4x on Intel CPUs
```

**Note:** Current implementation is a mock. Real OpenVINO integration:
```bash
pip install openvino
```

### 5.4 State Management

Persist state across executions.

**Using StateManager:**

```python
from aiwork.memory.state_manager import StateManager

# Local state (file-based)
state = StateManager(use_redis=False)

# Save state
state.save("flow_123", {
    "status": "running",
    "completed_tasks": ["task1", "task2"],
    "current_task": "task3"
})

# Load state
flow_state = state.load("flow_123")
print(flow_state["completed_tasks"])

# Redis state (distributed)
state = StateManager(
    use_redis=True,
    redis_url="redis://localhost:6379"
)
```

**Resume Interrupted Flows:**

```python
def execute_with_resume(flow, flow_id, initial_context):
    state_mgr = StateManager()
    
    # Check for existing state
    saved_state = state_mgr.load(flow_id)
    
    if saved_state:
        print(f"Resuming flow from task: {saved_state['current_task']}")
        # Resume from checkpoint
        context = saved_state["context"]
    else:
        context = initial_context
    
    # Execute with checkpointing
    orch = Orchestrator()
    result = orch.execute(flow, context)
    
    # Clear state on completion
    state_mgr.delete(flow_id)
    
    return result
```

---

## 6. üí° Best Practices

### 6.1 Agent Design

**‚úÖ DO:**
- Give agents specific, focused roles
- Provide clear goals and context
- Use backstory to guide behavior
- Equip with relevant tools only

**‚ùå DON'T:**
- Create "do everything" agents
- Use vague or conflicting goals
- Give unnecessary tools (noise)
- Make backstories too long

**Example:**

```python
# ‚úÖ GOOD: Focused agent
ocr_agent = Agent(
    role="OCR Specialist",
    goal="Extract text accurately from images",
    backstory="Trained on millions of documents, 99% accuracy",
    tools=[openvino_ocr]
)

# ‚ùå BAD: Unfocused agent
general_agent = Agent(
    role="General AI",
    goal="Do everything",
    backstory="Jack of all trades...",
    tools=[ocr, nlp, database, api, calculator, ...]  # Too many!
)
```

### 6.2 Task Composition

**‚úÖ DO:**
- Keep tasks atomic (single responsibility)
- Use clear, descriptive names
- Set appropriate retry counts
- Add guardrails for critical tasks

**‚ùå DON'T:**
- Create mega-tasks that do many things
- Use generic names like "process" or "handle"
- Set retries to 0 for unreliable operations
- Skip validation on sensitive outputs

**Example:**

```python
# ‚úÖ GOOD: Atomic tasks
flow.add_task(Task("extract_text", extract_handler, retries=3))
flow.add_task(Task("validate_format", validate_handler, retries=1))
flow.add_task(Task("detect_fraud", fraud_handler, retries=2))
flow.add_task(Task("store_result", store_handler, retries=5))

# ‚ùå BAD: Monolithic task
flow.add_task(Task(
    "do_everything",
    lambda ctx: extract_validate_analyze_store(ctx),
    retries=1
))
```

### 6.3 Flow Design

**‚úÖ DO:**
- Design flows as DAGs (no cycles)
- Minimize dependencies between tasks
- Use parallel branches where possible
- Keep flows focused on one workflow

**‚ùå DON'T:**
- Create circular dependencies
- Make every task depend on all previous tasks
- Mix unrelated workflows in one flow
- Create overly deep chains (>10 levels)

**Example:**

```python
# ‚úÖ GOOD: Parallel execution
flow.add_task(Task("extract", extract_handler))
flow.add_task(Task("sentiment", sentiment_handler), depends_on=["extract"])
flow.add_task(Task("entities", entity_handler), depends_on=["extract"])
flow.add_task(Task("topics", topic_handler), depends_on=["extract"])
# sentiment, entities, topics can run in parallel (future feature)

# ‚ùå BAD: Unnecessary sequencing
flow.add_task(Task("extract", extract_handler))
flow.add_task(Task("sentiment", sentiment_handler), depends_on=["extract"])
flow.add_task(Task("entities", entity_handler), depends_on=["sentiment"])
flow.add_task(Task("topics", topic_handler), depends_on=["entities"])
# Forces sequential when tasks are independent
```

### 6.4 Error Handling

**‚úÖ DO:**
- Set retries for transient failures
- Use guardrails for validation
- Log errors with context
- Handle exceptions gracefully

**‚ùå DON'T:**
- Ignore errors silently
- Set infinite retries
- Skip validation to "move fast"
- Crash the entire flow on one failure

**Example:**

```python
# ‚úÖ GOOD: Robust error handling
def api_call_handler(ctx):
    try:
        result = call_external_api(ctx["data"])
        return {"status": "success", "data": result}
    except APITimeout:
        # Transient error - let retry handle it
        raise
    except APIError as e:
        # Permanent error - return error state
        return {"status": "error", "message": str(e)}

task = Task(
    "api_call",
    api_call_handler,
    retries=3,  # Retry transient failures
    guardrails=[response_validator]  # Validate success
)

# ‚ùå BAD: No error handling
def bad_handler(ctx):
    result = call_external_api(ctx["data"])  # Can crash
    return result  # No validation

task = Task("api_call", bad_handler, retries=0)  # No retries
```

### 6.5 Performance Optimization

**‚úÖ DO:**
- Use OpenVINO for ML workloads
- Cache expensive computations
- Minimize data passing between tasks
- Profile and optimize bottlenecks

**‚ùå DON'T:**
- Load models in every task execution
- Pass large objects in context
- Ignore obvious bottlenecks
- Optimize prematurely

**Example:**

```python
# ‚úÖ GOOD: Reuse model
ov_adapter = OpenVINOAdapter("models/bert.xml")  # Load once

def classify_handler(ctx):
    # Reuse loaded model
    result = ov_adapter.infer(ctx["data"])
    return {"class": result}

# ‚ùå BAD: Load model every time
def bad_handler(ctx):
    ov = OpenVINOAdapter("models/bert.xml")  # Loads on every call!
    result = ov.infer(ctx["data"])
    return {"class": result}
```

---

## 7. üîß Troubleshooting

### Common Issues

#### Issue 1: "Cycle detected in Flow DAG"

**Cause:** Circular dependencies in task graph.

**Solution:**
```python
# ‚ùå BAD: Circular dependency
flow.add_task(Task("a", handler_a), depends_on=["b"])
flow.add_task(Task("b", handler_b), depends_on=["a"])  # Cycle!

# ‚úÖ GOOD: Linear or branching
flow.add_task(Task("a", handler_a))
flow.add_task(Task("b", handler_b), depends_on=["a"])
```

#### Issue 2: "Task has no Agent and no Handler"

**Cause:** Task created without handler function.

**Solution:**
```python
# ‚ùå BAD
task = Task("process")  # No handler!

# ‚úÖ GOOD
task = Task("process", handler=my_function)
# OR with agent
task = Task("process", agent=my_agent, handler=my_function)
```

#### Issue 3: Task keeps failing/retrying

**Cause:** Task logic has permanent error or guardrail always fails.

**Solution:**
```python
# Debug by adding logging
def debug_handler(ctx):
    print(f"Context: {ctx}")
    try:
        result = process(ctx)
        print(f"Result: {result}")
        return result
    except Exception as e:
        print(f"Error: {e}")
        raise

# Check guardrails
task = Task("process", debug_handler, guardrails=[my_guard], retries=1)
```

#### Issue 4: Memory not working/returning wrong results

**Cause:** VectorMemory uses simple similarity - not semantic embeddings.

**Solution:**
```python
# VectorMemory is basic - for production use proper embeddings
# Current: just string matching
memory.add("The user likes Python")
results = memory.search("Python programming")  # Works

results = memory.search("coding in Python")  # May not match well

# For better results: integrate proper embedding models
```

#### Issue 5: Slow execution on large flows

**Cause:** Sequential execution of tasks.

**Workaround:**
```python
# Current: All tasks run sequentially
# Future: Parallel execution for independent tasks

# Optimize by:
# 1. Minimize dependencies
# 2. Cache results
# 3. Use faster tools/models
# 4. Profile bottlenecks with:

import time
def profiled_handler(ctx):
    start = time.time()
    result = actual_work(ctx)
    print(f"Task took {time.time() - start:.2f}s")
    return result
```

---

## 8. ‚ùì FAQ

### General Questions

**Q: Is AIWork production-ready?**

A: The core framework is production-ready and tested. However:
- ‚úÖ Core (Agent, Task, Flow, Orchestrator): Production-ready
- ‚úÖ REST API: Production-ready
- ‚ö†Ô∏è OpenVINO integration: Mock (proof-of-concept)
- ‚ö†Ô∏è Kafka integration: Stub (interface defined)

See [roadmap](ROADMAP.md) for full implementation timeline.

**Q: Wait, OpenVINO and Kafka are stubs? What does that mean?**

A: Yes, and this is **intentional**! Here's what it means:

**OpenVINO Adapter** (Stub):
- ‚úÖ Interface is defined and working
- ‚úÖ Shows how to integrate OpenVINO
- ‚úÖ Demonstrates performance patterns
- ‚ùå Doesn't compile real models (yet)
- ‚ùå Doesn't perform hardware acceleration (yet)

**Kafka Adapter** (Stub):
- ‚úÖ Interface is defined and working
- ‚úÖ Shows how distributed processing works
- ‚úÖ Returns mock data for testing
- ‚ùå Doesn't connect to real Kafka (yet)
- ‚ùå Doesn't distribute across workers (yet)

**Redis State Manager** (Partial):
- ‚úÖ Local storage fully functional
- ‚úÖ Interface for Redis defined
- ‚ùå Redis integration not implemented (yet)

**Why stubs?** We shipped v0.1.0 with clear interfaces to:
1. Get community feedback early
2. Keep dependencies minimal
3. Focus on core framework quality
4. Meet Intel Challenge deadline
5. Provide clear upgrade path

**Full details**: See [MOCK_IMPLEMENTATIONS.md](MOCK_IMPLEMENTATIONS.md)

**Q: Are the benchmarks real or simulated?**

A: **Current benchmarks are simulated** for v0.1.0:
- OpenVINO integration is a stub, so we simulate the expected 3.7x speedup
- Core framework performance (task execution, memory operations) is real
- Real benchmarks will be published after OpenVINO implementation (Q1 2025)

We're transparent about this in [BENCHMARKS.md](BENCHMARKS.md). The simulated results are based on:
- Published OpenVINO benchmark data
- Conservative estimates
- Real hardware capabilities

**Real benchmarks coming**: March 2025 on Intel DevCloud hardware.

**Q: When will the real implementations be ready?**

A: **Clear timeline** for Phase 1 (Real Integrations):

| Component | Timeline | Status |
|-----------|----------|--------|
| OpenVINO | Week 1-3 (Jan-Feb 2025) | üü° Planning |
| Kafka | Week 4-5 (Feb 2025) | üü° Planning |
| Redis | Week 6 (Feb 2025) | üü° Planning |
| **All Complete** | **March 2025 (v0.5.0)** | **üéØ Target** |

**Total effort**: ~6 weeks for full production implementations.

**Contributing?** We welcome help! See [CONTRIBUTING.md](../CONTRIBUTING.md)

**Q: Can I use AIWork in production today despite the stubs?**

A: **Yes, absolutely!** Here's how:

**Production-Ready Today** (no changes needed):
- ‚úÖ Core framework (Agents, Tasks, Flows)
- ‚úÖ REST API server
- ‚úÖ Local state management
- ‚úÖ Vector memory
- ‚úÖ Guardrails
- ‚úÖ Observability

**Workarounds for Stubs**:
- **Instead of OpenVINO**: Use native model inference (PyTorch, TensorFlow)
- **Instead of Kafka**: Use REST API for job submission
- **Instead of Redis**: Use local state (works for single-instance)

**Migration Path**:
- Start with production-ready components now
- Plan upgrade to real integrations in Q1 2025
- We'll provide migration guides and backward compatibility

**Full guide**: See [PRODUCTION_GUIDE.md](PRODUCTION_GUIDE.md) for step-by-step instructions.

**Q: How do I know what's production-ready vs. stub?**

A: Check these resources:

1. **[MOCK_IMPLEMENTATIONS.md](MOCK_IMPLEMENTATIONS.md)**: Complete transparency about every component
2. **[PRODUCTION_GUIDE.md](PRODUCTION_GUIDE.md)**: Migration steps for stubs
3. **Source code**: All stubs have clear comments like `# Real implementation would use:`
4. **This FAQ**: Summary of component status

**Quick Reference**:
- ‚úÖ = Production-ready (use with confidence)
- ‚ö†Ô∏è = Partial/Stub (understand limitations)
- ‚ùå = Not implemented (coming soon)

**Q: How does AIWork compare to LangChain/CrewAI?**

A: 
- **Simpler**: ~2000 lines vs. 50k+ lines
- **Faster**: Minimal dependencies, no heavy framework overhead
- **Transparent**: You can read and understand all the code
- **Intel Optimized**: Built for Intel hardware from day one
- **Learning-Friendly**: Great for understanding how agent frameworks work

Trade-off: Fewer batteries included, but easier to customize.

**Q: Do I need Intel hardware?**

A: No, but you get best performance on Intel CPUs/GPUs:
- Works on any hardware (AMD, ARM, etc.)
- OpenVINO optimizations require Intel hardware
- Benchmarks are measured on Intel Xeon

**Q: Can I use real LLMs (GPT, Claude, etc.)?**

A: Yes! Integrate via tools:

```python
def gpt_tool(prompt):
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content

agent = Agent(
    role="Writer",
    tools=[gpt_tool],
    ...
)
```

### Technical Questions

**Q: Can tasks run in parallel?**

A: Not yet. Current version executes tasks sequentially. Parallel execution is planned for Phase 2 (see roadmap).

**Q: How do I debug agent decisions?**

A: Set `verbose=True` on agents:

```python
agent = Agent(role="Analyst", verbose=True, ...)
# Prints agent thinking process during execution
```

**Q: Can I deploy AIWork in production?**

A: Yes, recommended patterns:
1. REST API behind load balancer
2. Kafka for distributed processing
3. Redis for shared state
4. Docker for containerization

See [DEPLOYMENT.md](DEPLOYMENT.md) for details.

**Q: How do I add custom tools?**

A: Tools are just Python functions:

```python
def my_custom_tool(input_data):
    # Your logic here
    return {"result": "processed"}

agent = Agent(
    role="Worker",
    tools=[my_custom_tool, another_tool],
    ...
)
```

**Q: Is there a GUI for building flows?**

A: Not yet. Planned for Phase 3. Current approach is code-first.

**Q: How do I monitor flows in production?**

A: Use the built-in metrics:

```python
from aiwork.core.observability import metrics

# After execution
task_metrics = metrics.get("task_duration_seconds")
for metric in task_metrics:
    print(f"Task: {metric['tags']['task']}")
    print(f"Duration: {metric['value']}s")
    print(f"Status: {metric['tags']['status']}")
```

### Troubleshooting Questions

**Q: Why is my agent not using memory?**

A: Check that:
1. Memory is attached to agent: `Agent(memory=my_memory, ...)`
2. Memory has entries: `memory.add("info")`
3. Agent is assigned to task: `Task(agent=my_agent, ...)`

**Q: Why are my tasks not running in the right order?**

A: Check dependencies:

```python
# Debug: Print execution order
flow = Flow("test")
# ... add tasks ...
sorted_tasks = flow.get_topological_sort()
for task in sorted_tasks:
    print(f"{task.name} depends on {flow.dependencies[task.name]}")
```

**Q: How do I reset/clear state?**

A: 
```python
from aiwork.memory.state_manager import StateManager

state = StateManager()
state.delete("flow_id")  # Clear specific flow
# OR
state = StateManager()  # Create fresh instance
```

---

## üéØ Next Steps

Now that you understand AIWork, here's what to do next:

1. **Experiment**: Run the examples in `examples/agents/`
2. **Build**: Create your own agent for your use case
3. **Optimize**: Use OpenVINO for ML workloads (on Intel hardware)
4. **Deploy**: Follow [DEPLOYMENT.md](DEPLOYMENT.md) to go to production
5. **Contribute**: Check [CONTRIBUTING.md](../CONTRIBUTING.md) to help improve AIWork

**Happy Building! üöÄ**

---

## üìö Additional Resources

- [Architecture Guide](ARCHITECTURE.md) - System internals
- [API Reference](API_REFERENCE.md) - Detailed API docs
- [Mock Implementations](MOCK_IMPLEMENTATIONS.md) - **Transparency about stubs vs. production**
- [Production Guide](PRODUCTION_GUIDE.md) - **Migration steps for real integrations**
- [Benchmarks](BENCHMARKS.md) - Performance data
- [Roadmap](ROADMAP.md) - Future plans
- [GitHub Repo](https://github.com/JayeshCC/Aiwork) - Source code

---

<div align="center">
  <sub>Built with ‚ù§Ô∏è for the Intel AI Innovation Challenge 2024</sub>
</div>
